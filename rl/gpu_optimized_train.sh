#!/bin/bash

# RTX 4080 ìµœì í™” í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ RTX 4080 ìµœì í™” í•™ìŠµ ì‹œì‘"
echo "================================"

# GPU ì •ë³´ ì¶œë ¥
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv
echo "================================"

# CUDA ìµœì í™” í™˜ê²½ ë³€ìˆ˜
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_LAUNCH_BLOCKING=0

# í…ì„œ ì½”ì–´ í™œì„±í™”
export TORCH_CUDNN_V8_API_ENABLED=1
export CUDNN_BENCHMARK=1

# ì‹œìŠ¤í…œ ìµœì í™”
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

# í•™ìŠµ ëª¨ë“œ ì„ íƒ
echo "í•™ìŠµ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:"
echo "1) ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ (2-3ì‹œê°„)"
echo "2) í‘œì¤€ í•™ìŠµ (5-6ì‹œê°„)"
echo "3) ì •ë°€ í•™ìŠµ (10-12ì‹œê°„)"
echo "4) ì‚¬ìš©ì ì •ì˜"
read -p "ì„ íƒ (1-4): " mode

case $mode in
    1)
        echo "âš¡ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ ëª¨ë“œ"
        uv run python train.py \
            --mode train \
            --total_timesteps 3000000 \
            --rollout_length 4096 \
            --batch_size 256 \
            --lr 8e-4 \
            --ppo_epochs 8 \
            --save_freq 50 \
            --wandb
        ;;
    2)
        echo "ğŸ¯ í‘œì¤€ í•™ìŠµ ëª¨ë“œ"
        uv run python train.py \
            --mode train \
            --total_timesteps 8000000 \
            --rollout_length 8192 \
            --batch_size 512 \
            --lr 5e-4 \
            --ppo_epochs 12 \
            --save_freq 100 \
            --wandb
        ;;
    3)
        echo "ğŸ’ ì •ë°€ í•™ìŠµ ëª¨ë“œ"
        uv run python train.py \
            --mode train \
            --total_timesteps 20000000 \
            --rollout_length 16384 \
            --batch_size 1024 \
            --lr 3e-4 \
            --lr_schedule linear \
            --ppo_epochs 20 \
            --gamma 0.995 \
            --gae_lambda 0.97 \
            --clip_ratio 0.15 \
            --save_freq 200 \
            --wandb
        ;;
    4)
        echo "ğŸ”§ ì‚¬ìš©ì ì •ì˜ ëª¨ë“œ"
        read -p "Total timesteps (default: 10000000): " timesteps
        read -p "Batch size (default: 512): " batch
        read -p "Learning rate (default: 5e-4): " lr
        
        timesteps=${timesteps:-10000000}
        batch=${batch:-512}
        lr=${lr:-5e-4}
        
        uv run python train.py \
            --mode train \
            --total_timesteps $timesteps \
            --rollout_length 8192 \
            --batch_size $batch \
            --lr $lr \
            --ppo_epochs 15 \
            --save_freq 100 \
            --wandb
        ;;
esac

echo "================================"
echo "í•™ìŠµ ì™„ë£Œ!"
echo "ëª¨ë¸ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´:"
echo "uv run python train.py --mode eval --render"