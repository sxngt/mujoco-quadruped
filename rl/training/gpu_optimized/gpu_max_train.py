#!/usr/bin/env python3
"""
RTX 4080 GPU ìµœëŒ€ í™œìš© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
GPU ì‚¬ìš©ë¥  80% ëª©í‘œë¡œ ìµœì í™”ëœ ì„¤ì •
"""

import numpy as np
import torch
import time
import os
import argparse
from datetime import datetime
from simple_vectorized import SimpleVectorEnv
from ppo_agent import PPOAgent
import wandb


class GPUMaxTrainer:
    """RTX 4080 ìµœëŒ€ í™œìš© íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, args):
        self.args = args
        
        # GPU ìµœì í™” ì„¤ì •
        self._setup_gpu_optimization()
        
        # ëŒ€ìš©ëŸ‰ ë²¡í„°í™” í™˜ê²½ ìƒì„±
        self.vec_env = SimpleVectorEnv(
            num_envs=args.num_envs,
            render_mode="human" if args.render else None,
            use_reference_gait=not args.no_reference_gait
        )
        
        # GPU ìµœì í™”ëœ PPO ì—ì´ì „íŠ¸
        self.agent = PPOAgent(
            obs_dim=self.vec_env.observation_space.shape[0],
            action_dim=self.vec_env.action_space.shape[0],
            lr=args.lr,
            gamma=args.gamma,
            gae_lambda=args.gae_lambda,
            clip_ratio=args.clip_ratio,
            hidden_dim=args.hidden_dim  # ë” í° ë„¤íŠ¸ì›Œí¬
        )
        
        # í˜¼í•© ì •ë°€ë„ í›ˆë ¨ ì„¤ì •
        if args.mixed_precision:
            self.scaler = torch.amp.GradScaler('cuda')
            print("ğŸ”¥ í˜¼í•© ì •ë°€ë„ í›ˆë ¨ í™œì„±í™”")
        else:
            self.scaler = None
        
        # ë¡œê¹… ì´ˆê¸°í™”
        if args.wandb:
            wandb.init(
                project="go2-gpu-max-locomotion",
                config=vars(args),
                name=f"gpu_max_{args.num_envs}envs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
        
        # í†µê³„ ë³€ìˆ˜
        self.episode_rewards = [[] for _ in range(args.num_envs)]
        self.episode_lengths = [[] for _ in range(args.num_envs)]
        self.episode_counts = np.zeros(args.num_envs, dtype=int)
        self.best_reward = -np.inf
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.gpu_memory_peak = 0
        self.fps_history = []
        
        print(f"ğŸš€ GPU ìµœëŒ€ í™œìš© í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ")
        print(f"í™˜ê²½ ìˆ˜: {args.num_envs}")
        print(f"ë””ë°”ì´ìŠ¤: {self.agent.device}")
        print(f"ë„¤íŠ¸ì›Œí¬ í¬ê¸°: {args.hidden_dim}")
        print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
        print(f"ë¡¤ì•„ì›ƒ ê¸¸ì´: {args.rollout_length}")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹
        self._warmup_gpu()
    
    def _setup_gpu_optimization(self):
        """GPU ìµœì í™” ì„¤ì •"""
        if torch.cuda.is_available():
            # CUDA ìµœì í™” ì„¤ì •
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # RTX 4080 ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™” (16GB)
            torch.cuda.empty_cache()
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024,expandable_segments:True,garbage_collection_threshold:0.8'
            
            print(f"ğŸ”§ GPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
            print(f"GPU: {torch.cuda.get_device_name()}")
            print(f"CUDA ë²„ì „: {torch.version.cuda}")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    def _warmup_gpu(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹ ë° ì›Œë°ì—…"""
        print("ğŸ”¥ GPU ì›Œë°ì—… ì‹œì‘...")
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ GPU ì›Œë°ì—…
        dummy_obs = torch.randn(
            self.args.batch_size, 
            self.vec_env.observation_space.shape[0], 
            device=self.agent.device
        )
        dummy_actions = torch.randn(
            self.args.batch_size, 
            self.vec_env.action_space.shape[0], 
            device=self.agent.device
        )
        
        # ëª‡ ë²ˆì˜ forward/backward passë¡œ ì›Œë°ì—…
        for _ in range(10):
            with torch.autocast(device_type='cuda', enabled=self.args.mixed_precision):
                _ = self.agent.policy(dummy_obs)
            
        torch.cuda.synchronize()
        initial_memory = torch.cuda.memory_allocated() / 1e9
        print(f"ğŸ’¾ ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.2f}GB")
    
    def get_vectorized_actions_optimized(self, observations):
        """GPU ìµœì í™”ëœ ë²¡í„°í™” í–‰ë™ ìƒì„±"""
        obs_tensor = torch.FloatTensor(observations).to(self.agent.device, non_blocking=True)
        
        with torch.no_grad():
            with torch.autocast(device_type='cuda', enabled=self.args.mixed_precision):
                actions, log_probs, values = self.agent.policy.get_action(obs_tensor)
        
        return actions.cpu().numpy(), log_probs.cpu().numpy(), values.cpu().numpy()
    
    def collect_rollout_optimized(self):
        """GPU ìµœì í™”ëœ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        observations, infos = self.vec_env.reset(seed=self.args.seed)
        current_episode_rewards = np.zeros(self.args.num_envs)
        current_episode_lengths = np.zeros(self.args.num_envs, dtype=int)
        
        rollout_start_time = time.time()
        steps_collected = 0
        target_steps = self.args.rollout_length
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ìˆ˜ì§‘
        obs_batch = []
        action_batch = []
        reward_batch = []
        value_batch = []
        logprob_batch = []
        done_batch = []
        
        while steps_collected < target_steps:
            # í–‰ë™ ìƒì„± (GPU ê°€ì†)
            actions, log_probs, values = self.get_vectorized_actions_optimized(observations)
            
            # í™˜ê²½ ìŠ¤í…
            next_observations, rewards, terminated, truncated, infos = self.vec_env.step(actions)
            dones = terminated | truncated
            
            # ë°°ì¹˜ì— ì¶”ê°€
            obs_batch.append(observations.copy())
            action_batch.append(actions.copy())
            reward_batch.append(rewards.copy())
            value_batch.append(values.copy())
            logprob_batch.append(log_probs.copy())
            done_batch.append(dones.copy())
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            observations = next_observations
            current_episode_rewards += rewards
            current_episode_lengths += 1
            steps_collected += self.args.num_envs
            
            # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬
            for env_idx in range(self.args.num_envs):
                if dones[env_idx]:
                    self.episode_rewards[env_idx].append(current_episode_rewards[env_idx])
                    self.episode_lengths[env_idx].append(current_episode_lengths[env_idx])
                    self.episode_counts[env_idx] += 1
                    
                    current_episode_rewards[env_idx] = 0
                    current_episode_lengths[env_idx] = 0
            
            # ë Œë”ë§
            if self.args.render and steps_collected % (self.args.num_envs * 4) == 0:
                self.vec_env.render()
        
        # ë°°ì¹˜ ë°ì´í„°ë¥¼ ì—ì´ì „íŠ¸ì— ì €ì¥
        for i in range(len(obs_batch)):
            for j in range(self.args.num_envs):
                self.agent.store_transition(
                    obs_batch[i][j], action_batch[i][j], reward_batch[i][j],
                    value_batch[i][j], logprob_batch[i][j], done_batch[i][j]
                )
        
        rollout_time = time.time() - rollout_start_time
        return steps_collected, rollout_time
    
    def update_policy_optimized(self):
        """GPU ìµœì í™”ëœ ì •ì±… ì—…ë°ì´íŠ¸"""
        update_start_time = time.time()
        
        # ë” í° ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸
        losses = self.agent.update_policy(
            n_epochs=self.args.ppo_epochs,
            batch_size=self.args.batch_size
        )
        
        update_time = time.time() - update_start_time
        
        # GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        current_memory = torch.cuda.memory_allocated() / 1e9
        self.gpu_memory_peak = max(self.gpu_memory_peak, current_memory)
        
        return losses, update_time
    
    def train(self):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        start_time = time.time()
        total_timesteps = 0
        
        print(f"ğŸ¯ GPU ìµœëŒ€ í™œìš© í•™ìŠµ ì‹œì‘ (ëª©í‘œ: {self.args.total_timesteps:,} ìŠ¤í…)")
        
        while total_timesteps < self.args.total_timesteps:
            # ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
            steps_collected, rollout_time = self.collect_rollout_optimized()
            total_timesteps += steps_collected
            
            # ì •ì±… ì—…ë°ì´íŠ¸  
            losses, update_time = self.update_policy_optimized()
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            fps = steps_collected / (rollout_time + update_time)
            self.fps_history.append(fps)
            
            # í†µê³„ ë¡œê¹…
            self.log_progress_optimized(total_timesteps, start_time, rollout_time, update_time, fps, losses)
            
            # ëª¨ë¸ ì €ì¥
            self.save_models(total_timesteps)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì )
            if total_timesteps % (self.args.save_freq * 2) < self.args.num_envs:
                torch.cuda.empty_cache()
        
        # ìµœì¢… ì •ë¦¬
        self.cleanup()
        
        total_time = time.time() - start_time
        avg_fps = np.mean(self.fps_history[-100:]) if self.fps_history else 0
        
        print(f"âœ… GPU ìµœëŒ€ í™œìš© í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì‹œê°„: {total_time/3600:.1f}ì‹œê°„")
        print(f"í‰ê·  FPS (ìµœê·¼ 100íšŒ): {avg_fps:.0f}")
        print(f"ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {self.gpu_memory_peak:.2f}GB")
        print(f"ìµœê³  ì„±ëŠ¥: {self.best_reward:.2f}")
    
    def log_progress_optimized(self, total_timesteps, start_time, rollout_time, update_time, fps, losses):
        """ìµœì í™”ëœ ì§„í–‰ìƒí™© ë¡œê¹…"""
        # í†µê³„ ê³„ì‚°
        all_rewards = []
        all_lengths = []
        
        for env_rewards in self.episode_rewards:
            all_rewards.extend(env_rewards[-5:])  # ìµœê·¼ 5ê°œ ì—í”¼ì†Œë“œ
        for env_lengths in self.episode_lengths:
            all_lengths.extend(env_lengths[-5:])
        
        if all_rewards:
            avg_reward = np.mean(all_rewards)
            avg_length = np.mean(all_lengths)
            total_episodes = np.sum(self.episode_counts)
            
            elapsed_time = time.time() - start_time
            overall_fps = total_timesteps / elapsed_time
            
            # GPU í™œìš©ë¥  ì¶”ì • (ê°„ì ‘ì )
            gpu_memory_gb = torch.cuda.memory_allocated() / 1e9
            estimated_gpu_util = min(95, fps / 50 * 100)  # ì¶”ì •ì¹˜
            
            print(f"ìŠ¤í… {total_timesteps:8,} | ì—í”¼ì†Œë“œ {total_episodes:5,} | "
                  f"í‰ê·  ë³´ìƒ: {avg_reward:7.2f} | í‰ê·  ê¸¸ì´: {avg_length:5.1f} | "
                  f"FPS: {fps:5.0f} | GPU: {estimated_gpu_util:3.0f}% | "
                  f"ë©”ëª¨ë¦¬: {gpu_memory_gb:.1f}GB | "
                  f"ë¡¤ì•„ì›ƒ: {rollout_time:.2f}s | ì—…ë°ì´íŠ¸: {update_time:.2f}s")
            
            # Weights & Biases ë¡œê¹…
            if self.args.wandb:
                log_dict = {
                    "timestep": total_timesteps,
                    "total_episodes": total_episodes,
                    "avg_reward": avg_reward,
                    "avg_length": avg_length,
                    "fps": fps,
                    "overall_fps": overall_fps,
                    "estimated_gpu_util": estimated_gpu_util,
                    "gpu_memory_gb": gpu_memory_gb,
                    "rollout_time": rollout_time,
                    "update_time": update_time,
                    "num_envs": self.args.num_envs,
                }
                if losses:
                    log_dict.update(losses)
                wandb.log(log_dict)
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if avg_reward > self.best_reward:
                self.best_reward = avg_reward
                if not os.path.exists("models"):
                    os.makedirs("models")
                self.agent.save(f"models/best_gpu_max_go2_ppo.pth")
                print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! í‰ê·  ë³´ìƒ: {self.best_reward:.2f}")
    
    def save_models(self, total_timesteps):
        """ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥"""
        if total_timesteps % self.args.save_freq < self.args.num_envs:
            if not os.path.exists("models"):
                os.makedirs("models")
            self.agent.save(f"models/gpu_max_go2_ppo_step_{total_timesteps}.pth")
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {total_timesteps:,} ìŠ¤í…")
    
    def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        if not os.path.exists("models"):
            os.makedirs("models")
        self.agent.save("models/gpu_max_go2_ppo_final.pth")
        
        # í™˜ê²½ ì¢…ë£Œ
        self.vec_env.close()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        # Weights & Biases ì¢…ë£Œ
        if self.args.wandb:
            wandb.finish()


def main():
    parser = argparse.ArgumentParser(description='RTX 4080 GPU ìµœëŒ€ í™œìš© í•™ìŠµ')
    
    # RTX 4080 GPU ìµœì í™” ì„¤ì • (16GB VRAM)
    parser.add_argument('--num_envs', type=int, default=96, 
                        help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (RTX 4080 16GB ê¸°ì¤€ 96ê°œ ìµœì í™”)')
    parser.add_argument('--mixed_precision', action='store_true', default=True,
                        help='í˜¼í•© ì •ë°€ë„ í›ˆë ¨ (ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ)')
    parser.add_argument('--hidden_dim', type=int, default=768,
                        help='ë„¤íŠ¸ì›Œí¬ íˆë“  ë ˆì´ì–´ í¬ê¸° (RTX 4080 GPU í™œìš©ë„ ìµœëŒ€í™”)')
    
    # ëŒ€ìš©ëŸ‰ í•™ìŠµ ì„¤ì •
    parser.add_argument('--total_timesteps', type=int, default=20000000,
                        help='ì´ í•™ìŠµ ìŠ¤í… (2ì²œë§Œ)')
    parser.add_argument('--rollout_length', type=int, default=24576,
                        help='ë¡¤ì•„ì›ƒ ê¸¸ì´ (RTX 4080 16GB ëŒ€ìš©ëŸ‰ ìµœì í™”)')
    parser.add_argument('--batch_size', type=int, default=3072,
                        help='ë°°ì¹˜ í¬ê¸° (RTX 4080 16GB ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©)')
    
    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--lr', type=float, default=1e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--gamma', type=float, default=0.99, help='í• ì¸ ì¸ìˆ˜')
    parser.add_argument('--gae_lambda', type=float, default=0.95, help='GAE lambda')
    parser.add_argument('--clip_ratio', type=float, default=0.2, help='PPO í´ë¦½ ë¹„ìœ¨')
    parser.add_argument('--ppo_epochs', type=int, default=20, help='PPO ì—…ë°ì´íŠ¸ ì—í¬í¬ (ë§ì´)')
    
    # í™˜ê²½ ì„¤ì •
    parser.add_argument('--no_reference_gait', action='store_true', default=True,
                        help='ì°¸ì¡° gait ë¹„í™œì„±í™” (ê¸°ë³¸)')
    parser.add_argument('--render', action='store_true',
                        help='ì²« ë²ˆì§¸ í™˜ê²½ ë Œë”ë§ (ì„±ëŠ¥ ì €í•˜)')
    
    # ë¡œê¹… ë° ì €ì¥
    parser.add_argument('--wandb', action='store_true', help='Weights & Biases ë¡œê¹…')
    parser.add_argument('--save_freq', type=int, default=500000,
                        help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸° (ìŠ¤í… ë‹¨ìœ„)')
    
    # ê¸°íƒ€
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ')
    
    args = parser.parse_args()
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ì‹œë“œ ì„¤ì •
    if args.seed is not None:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        torch.cuda.manual_seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
    
    print(f"ğŸš€ RTX 4080 GPU ìµœëŒ€ í™œìš© í•™ìŠµ ì‹œì‘!")
    print(f"ì„¤ì •: {args.num_envs}ê°œ í™˜ê²½, {args.total_timesteps:,} ìŠ¤í… ëª©í‘œ")
    print(f"ë„¤íŠ¸ì›Œí¬ í¬ê¸°: {args.hidden_dim}, ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"í˜¼í•© ì •ë°€ë„: {'í™œì„±í™”' if args.mixed_precision else 'ë¹„í™œì„±í™”'}")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ ì‹œì‘
    trainer = GPUMaxTrainer(args)
    trainer.train()


if __name__ == "__main__":
    main()