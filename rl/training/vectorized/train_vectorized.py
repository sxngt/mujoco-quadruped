#!/usr/bin/env python3
"""
ë²¡í„°í™” í™˜ê²½ì„ ì‚¬ìš©í•œ ê³ ì† PPO í•™ìŠµ
16ê°œ í™˜ê²½ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ í•™ìŠµ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
"""

import numpy as np
import torch
import time
import os
import argparse
from datetime import datetime
from simple_vectorized import SimpleVectorEnv
from ppo_agent import PPOAgent
import wandb


class VectorizedTrainer:
    """ë²¡í„°í™” í™˜ê²½ ì „ìš© PPO íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, args):
        self.args = args
        
        # ë²¡í„°í™” í™˜ê²½ ìƒì„±
        self.vec_env = SimpleVectorEnv(
            num_envs=args.num_envs,
            render_mode="human" if args.render else None,
            use_reference_gait=not args.no_reference_gait
        )
        
        # PPO ì—ì´ì „íŠ¸ ìƒì„±
        self.agent = PPOAgent(
            obs_dim=self.vec_env.observation_space.shape[0],
            action_dim=self.vec_env.action_space.shape[0],
            lr=args.lr,
            gamma=args.gamma,
            gae_lambda=args.gae_lambda,
            clip_ratio=args.clip_ratio
        )
        
        # ë¡œê¹… ì´ˆê¸°í™”
        if args.wandb:
            wandb.init(
                project="go2-vectorized-locomotion",
                config=vars(args),
                name=f"vec_ppo_{args.num_envs}envs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
        
        # í†µê³„ ë³€ìˆ˜
        self.episode_rewards = [[] for _ in range(args.num_envs)]
        self.episode_lengths = [[] for _ in range(args.num_envs)]
        self.episode_counts = np.zeros(args.num_envs, dtype=int)
        self.best_reward = -np.inf
        
        print(f"ğŸš€ ë²¡í„°í™” í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ")
        print(f"í™˜ê²½ ìˆ˜: {args.num_envs}")
        print(f"ë””ë°”ì´ìŠ¤: {self.agent.device}")
        print(f"ë¡¤ì•„ì›ƒ ê¸¸ì´: {args.rollout_length}")
        print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    
    def get_vectorized_actions(self, observations):
        """
        ë²¡í„°í™”ëœ ê´€ì°°ì— ëŒ€í•´ í–‰ë™ ìƒì„±
        
        Args:
            observations: (num_envs, obs_dim) ê´€ì°° ë°°ì—´
            
        Returns:
            actions: (num_envs, action_dim) í–‰ë™ ë°°ì—´
            log_probs: (num_envs,) ë¡œê·¸ í™•ë¥  ë°°ì—´  
            values: (num_envs,) ê°€ì¹˜ ì¶”ì • ë°°ì—´
        """
        obs_tensor = torch.FloatTensor(observations).to(self.agent.device)
        
        with torch.no_grad():
            # PPOAgentì˜ PolicyNetwork ì‚¬ìš©
            actions, log_probs, values = self.agent.policy.get_action(obs_tensor)
        
        return actions.cpu().numpy(), log_probs.cpu().numpy(), values.cpu().numpy()
    
    def collect_rollout(self):
        """ë¡¤ì•„ì›ƒ ë°ì´í„° ìˆ˜ì§‘"""
        observations, infos = self.vec_env.reset(seed=self.args.seed)
        current_episode_rewards = np.zeros(self.args.num_envs)
        current_episode_lengths = np.zeros(self.args.num_envs, dtype=int)
        
        steps_collected = 0
        target_steps = self.args.rollout_length
        
        while steps_collected < target_steps:
            # í–‰ë™ ìƒì„±
            actions, log_probs, values = self.get_vectorized_actions(observations)
            
            # í™˜ê²½ ìŠ¤í…
            next_observations, rewards, terminated, truncated, infos = self.vec_env.step(actions)
            dones = terminated | truncated
            
            # ë°ì´í„° ì €ì¥ (ê° í™˜ê²½ë³„ë¡œ)
            for i in range(self.args.num_envs):
                self.agent.store_transition(
                    observations[i], actions[i], rewards[i], 
                    values[i], log_probs[i], dones[i]
                )
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            observations = next_observations
            current_episode_rewards += rewards
            current_episode_lengths += 1
            steps_collected += self.args.num_envs
            
            # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬
            for env_idx in range(self.args.num_envs):
                if dones[env_idx]:
                    self.episode_rewards[env_idx].append(current_episode_rewards[env_idx])
                    self.episode_lengths[env_idx].append(current_episode_lengths[env_idx])
                    self.episode_counts[env_idx] += 1
                    
                    current_episode_rewards[env_idx] = 0
                    current_episode_lengths[env_idx] = 0
            
            # ë Œë”ë§
            if self.args.render:
                self.vec_env.render()
        
        return steps_collected
    
    def train(self):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        start_time = time.time()
        total_timesteps = 0
        
        print(f"ğŸ¯ í•™ìŠµ ì‹œì‘ (ëª©í‘œ: {self.args.total_timesteps:,} ìŠ¤í…)")
        
        while total_timesteps < self.args.total_timesteps:
            # ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
            rollout_start = time.time()
            steps_collected = self.collect_rollout()
            rollout_time = time.time() - rollout_start
            
            total_timesteps += steps_collected
            
            # ì •ì±… ì—…ë°ì´íŠ¸
            update_start = time.time()
            losses = self.agent.update_policy(
                n_epochs=self.args.ppo_epochs,
                batch_size=self.args.batch_size
            )
            update_time = time.time() - update_start
            
            # í†µê³„ ê³„ì‚°
            self.log_progress(total_timesteps, start_time, rollout_time, update_time, losses)
            
            # ëª¨ë¸ ì €ì¥
            self.save_models(total_timesteps)
        
        # ìµœì¢… ì •ë¦¬
        self.cleanup()
        
        total_time = time.time() - start_time
        final_fps = total_timesteps / total_time
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì‹œê°„: {total_time/3600:.1f}ì‹œê°„")
        print(f"í‰ê·  FPS: {final_fps:.0f}")
        print(f"ìµœê³  ì„±ëŠ¥: {self.best_reward:.2f}")
    
    def log_progress(self, total_timesteps, start_time, rollout_time, update_time, losses):
        """í•™ìŠµ ì§„í–‰ìƒí™© ë¡œê¹…"""
        # ìµœê·¼ ì—í”¼ì†Œë“œë“¤ì˜ í†µê³„
        all_rewards = []
        all_lengths = []
        
        for env_rewards in self.episode_rewards:
            all_rewards.extend(env_rewards[-10:])  # ìµœê·¼ 10ê°œ ì—í”¼ì†Œë“œ
        for env_lengths in self.episode_lengths:
            all_lengths.extend(env_lengths[-10:])
        
        if all_rewards:
            avg_reward = np.mean(all_rewards)
            avg_length = np.mean(all_lengths)
            total_episodes = np.sum(self.episode_counts)
            
            elapsed_time = time.time() - start_time
            fps = total_timesteps / elapsed_time
            
            print(f"ìŠ¤í… {total_timesteps:7,} | ì—í”¼ì†Œë“œ {total_episodes:4,} | "
                  f"í‰ê·  ë³´ìƒ: {avg_reward:7.2f} | í‰ê·  ê¸¸ì´: {avg_length:5.1f} | "
                  f"FPS: {fps:5.0f} | ë¡¤ì•„ì›ƒ: {rollout_time:.2f}s | ì—…ë°ì´íŠ¸: {update_time:.2f}s")
            
            # Weights & Biases ë¡œê¹…
            if self.args.wandb:
                log_dict = {
                    "timestep": total_timesteps,
                    "total_episodes": total_episodes,
                    "avg_reward": avg_reward,
                    "avg_length": avg_length,
                    "fps": fps,
                    "rollout_time": rollout_time,
                    "update_time": update_time,
                    "num_envs": self.args.num_envs,
                }
                if losses:
                    log_dict.update(losses)
                wandb.log(log_dict)
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if avg_reward > self.best_reward:
                self.best_reward = avg_reward
                if not os.path.exists("models"):
                    os.makedirs("models")
                self.agent.save(f"models/best_vectorized_go2_ppo.pth")
                print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! í‰ê·  ë³´ìƒ: {self.best_reward:.2f}")
    
    def save_models(self, total_timesteps):
        """ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥"""
        if total_timesteps % self.args.save_freq < self.args.num_envs:
            if not os.path.exists("models"):
                os.makedirs("models")
            self.agent.save(f"models/vectorized_go2_ppo_step_{total_timesteps}.pth")
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {total_timesteps:,} ìŠ¤í…")
    
    def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        if not os.path.exists("models"):
            os.makedirs("models")
        self.agent.save("models/vectorized_go2_ppo_final.pth")
        
        # í™˜ê²½ ì¢…ë£Œ
        self.vec_env.close()
        
        # Weights & Biases ì¢…ë£Œ
        if self.args.wandb:
            wandb.finish()


def main():
    parser = argparse.ArgumentParser(description='ë²¡í„°í™” PPO í•™ìŠµ')
    
    # ë²¡í„°í™” í™˜ê²½ ì„¤ì •
    parser.add_argument('--num_envs', type=int, default=16, 
                        help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (ê¸°ë³¸: 16)')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--total_timesteps', type=int, default=2000000,
                        help='ì´ í•™ìŠµ ìŠ¤í…')
    parser.add_argument('--rollout_length', type=int, default=4096,
                        help='ë¡¤ì•„ì›ƒ ê¸¸ì´')
    
    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--lr', type=float, default=3e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--gamma', type=float, default=0.99, help='í• ì¸ ì¸ìˆ˜')
    parser.add_argument('--gae_lambda', type=float, default=0.95, help='GAE lambda')
    parser.add_argument('--clip_ratio', type=float, default=0.2, help='PPO í´ë¦½ ë¹„ìœ¨')
    parser.add_argument('--ppo_epochs', type=int, default=10, help='PPO ì—…ë°ì´íŠ¸ ì—í¬í¬')
    parser.add_argument('--batch_size', type=int, default=256, help='ë°°ì¹˜ í¬ê¸°')
    
    # í™˜ê²½ ì„¤ì •
    parser.add_argument('--no_reference_gait', action='store_true',
                        help='ì°¸ì¡° gait ë¹„í™œì„±í™”')
    parser.add_argument('--render', action='store_true',
                        help='ì²« ë²ˆì§¸ í™˜ê²½ ë Œë”ë§')
    
    # ë¡œê¹… ë° ì €ì¥
    parser.add_argument('--wandb', action='store_true', help='Weights & Biases ë¡œê¹…')
    parser.add_argument('--save_freq', type=int, default=100000,
                        help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸° (ìŠ¤í… ë‹¨ìœ„)')
    
    # ê¸°íƒ€
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ')
    
    args = parser.parse_args()
    
    # ì‹œë“œ ì„¤ì •
    if args.seed is not None:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(args.seed)
    
    print(f"ğŸ® ë²¡í„°í™” PPO í•™ìŠµ ì‹œì‘")
    print(f"ì„¤ì •: {args.num_envs}ê°œ í™˜ê²½, {args.total_timesteps:,} ìŠ¤í… ëª©í‘œ")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ ì‹œì‘
    trainer = VectorizedTrainer(args)
    trainer.train()


if __name__ == "__main__":
    main()