#!/usr/bin/env python3
"""
ê°œì„ ëœ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ - Isaac Lab RSL-RL ê¸°ë²• ì ìš©
RTX 4080 GPU ìµœì í™” í¬í•¨
"""

import numpy as np
import torch
import time
import os
import argparse
from datetime import datetime
from environments.vectorized.improved_vectorized import ImprovedVectorEnv
from agents.ppo_agent import PPOAgent
import wandb


class ImprovedTrainer:
    """ê°œì„ ëœ GO2 í•™ìŠµ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, args):
        self.args = args
        
        # GPU ìµœì í™” ì„¤ì •
        self._setup_gpu_optimization()
        
        # ê°œì„ ëœ ë²¡í„°í™” í™˜ê²½ ìƒì„±
        self.vec_env = ImprovedVectorEnv(
            num_envs=args.num_envs,
            render_mode="human" if args.render else None,
            use_reference_gait=not args.no_reference_gait
        )
        
        # PPO ì—ì´ì „íŠ¸ (ê°œì„ ëœ í™˜ê²½ì— ë§ì¶¤)
        self.agent = PPOAgent(
            obs_dim=self.vec_env.observation_space.shape[0],
            action_dim=self.vec_env.action_space.shape[0],
            lr=args.lr,
            gamma=args.gamma,
            gae_lambda=args.gae_lambda,
            clip_ratio=args.clip_ratio,
            hidden_dim=args.hidden_dim
        )
        
        # í˜¼í•© ì •ë°€ë„ í›ˆë ¨
        if args.mixed_precision:
            self.scaler = torch.amp.GradScaler('cuda')
            print("ğŸ”¥ í˜¼í•© ì •ë°€ë„ í›ˆë ¨ í™œì„±í™”")
        else:
            self.scaler = None
        
        # ë¡œê¹… ì´ˆê¸°í™”
        if args.wandb:
            wandb.init(
                project="go2-improved-locomotion",
                config=vars(args),
                name=f"improved_{args.num_envs}envs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
        
        # í†µê³„ ë³€ìˆ˜
        self.episode_rewards = [[] for _ in range(args.num_envs)]
        self.episode_lengths = [[] for _ in range(args.num_envs)]
        self.episode_counts = np.zeros(args.num_envs, dtype=int)
        self.best_reward = -np.inf
        
        print(f"ğŸš€ ê°œì„ ëœ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ")
        print(f"í™˜ê²½ ìˆ˜: {args.num_envs}")
        print(f"ë””ë°”ì´ìŠ¤: {self.agent.device}")
        print(f"ì œì–´ ëª¨ë“œ: ê´€ì ˆ ìœ„ì¹˜ ì œì–´")
        print(f"ì´ˆê¸°í™”: ì°¸ì¡° ë³´í–‰ ìì„¸")
    
    def _setup_gpu_optimization(self):
        """GPU ìµœì í™” ì„¤ì •"""
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # RTX 4080 ë©”ëª¨ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024,expandable_segments:True'
            
            print(f"ğŸ”§ GPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
            print(f"GPU: {torch.cuda.get_device_name()}")
    
    def get_vectorized_actions(self, observations):
        """ë²¡í„°í™”ëœ í–‰ë™ ìƒì„±"""
        obs_tensor = torch.FloatTensor(observations).to(self.agent.device)
        
        with torch.no_grad():
            with torch.autocast(device_type='cuda', enabled=self.args.mixed_precision):
                actions, log_probs, values = self.agent.policy.get_action(obs_tensor)
        
        return actions.cpu().numpy(), log_probs.cpu().numpy(), values.cpu().numpy()
    
    def collect_rollout(self):
        """ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        observations, infos = self.vec_env.reset(seed=self.args.seed)
        current_episode_rewards = np.zeros(self.args.num_envs)
        current_episode_lengths = np.zeros(self.args.num_envs, dtype=int)
        
        rollout_start_time = time.time()
        steps_collected = 0
        target_steps = self.args.rollout_length
        
        # ë³´ìƒ êµ¬ì„± ìš”ì†Œ ì¶”ì 
        reward_components = {}
        
        while steps_collected < target_steps:
            # í–‰ë™ ìƒì„±
            actions, log_probs, values = self.get_vectorized_actions(observations)
            
            # í–‰ë™ í´ë¦¬í•‘ (ì•ˆì „ì„±)
            actions = np.clip(actions, -1.0, 1.0)
            
            # í™˜ê²½ ìŠ¤í…
            next_observations, rewards, terminated, truncated, infos = self.vec_env.step(actions)
            dones = terminated | truncated
            
            # ë³´ìƒ ì•ˆì •ì„± ê²€ì‚¬
            rewards = np.clip(rewards, -100.0, 100.0)
            rewards = np.where(np.isnan(rewards) | np.isinf(rewards), -10.0, rewards)
            
            # ì—ì´ì „íŠ¸ì— ì „ì´ ì €ì¥
            for env_idx in range(self.args.num_envs):
                self.agent.store_transition(
                    observations[env_idx], actions[env_idx], rewards[env_idx],
                    values[env_idx], log_probs[env_idx], dones[env_idx]
                )
            
            # ë³´ìƒ êµ¬ì„± ìš”ì†Œ ëˆ„ì 
            for env_idx, info in enumerate(infos):
                if isinstance(info, dict):
                    for key, value in info.items():
                        if key not in reward_components:
                            reward_components[key] = []
                        reward_components[key].append(value)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            observations = next_observations
            current_episode_rewards += rewards
            current_episode_lengths += 1
            steps_collected += self.args.num_envs
            
            # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬ (ìë™ ë¦¬ì…‹ ì—†ì´)
            reset_envs = []
            for env_idx in range(self.args.num_envs):
                if dones[env_idx]:
                    self.episode_rewards[env_idx].append(current_episode_rewards[env_idx])
                    self.episode_lengths[env_idx].append(current_episode_lengths[env_idx])
                    self.episode_counts[env_idx] += 1
                    
                    current_episode_rewards[env_idx] = 0
                    current_episode_lengths[env_idx] = 0
                    reset_envs.append(env_idx)
                    
                    print(f"í™˜ê²½ {env_idx} ì—í”¼ì†Œë“œ ì¢…ë£Œ: ë³´ìƒ {current_episode_rewards[env_idx]:.2f}, ê¸¸ì´ {current_episode_lengths[env_idx]}")
            
            # í•„ìš”ì‹œë§Œ ë¦¬ì…‹ (ë²¡í„°í™” í™˜ê²½ì—ì„œ ìë™ ì²˜ë¦¬ë¨)
            if len(reset_envs) > 0:
                print(f"{len(reset_envs)}ê°œ í™˜ê²½ì´ ìë™ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ë Œë”ë§
            if self.args.render and steps_collected % (self.args.num_envs * 10) == 0:
                self.vec_env.render()
        
        rollout_time = time.time() - rollout_start_time
        
        # í‰ê·  ë³´ìƒ êµ¬ì„± ìš”ì†Œ ê³„ì‚°
        avg_reward_components = {}
        for key, values in reward_components.items():
            avg_reward_components[key] = np.mean(values)
        
        return steps_collected, rollout_time, avg_reward_components
    
    def update_policy(self):
        """ì •ì±… ì—…ë°ì´íŠ¸"""
        update_start_time = time.time()
        
        losses = self.agent.update_policy(
            n_epochs=self.args.ppo_epochs,
            batch_size=self.args.batch_size
        )
        
        update_time = time.time() - update_start_time
        
        return losses, update_time
    
    def train(self):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        start_time = time.time()
        total_timesteps = 0
        
        print(f"ğŸ¯ ê°œì„ ëœ í•™ìŠµ ì‹œì‘ (ëª©í‘œ: {self.args.total_timesteps:,} ìŠ¤í…)")
        
        while total_timesteps < self.args.total_timesteps:
            # ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
            steps_collected, rollout_time, reward_components = self.collect_rollout()
            total_timesteps += steps_collected
            
            # ì •ì±… ì—…ë°ì´íŠ¸
            losses, update_time = self.update_policy()
            
            # í†µê³„ ë¡œê¹…
            self.log_progress(total_timesteps, start_time, rollout_time, update_time, 
                            losses, reward_components)
            
            # ëª¨ë¸ ì €ì¥
            self.save_models(total_timesteps)
        
        # ìµœì¢… ì •ë¦¬
        self.cleanup()
        
        total_time = time.time() - start_time
        print(f"âœ… ê°œì„ ëœ í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì‹œê°„: {total_time/3600:.1f}ì‹œê°„")
        print(f"ìµœê³  ì„±ëŠ¥: {self.best_reward:.2f}")
    
    def log_progress(self, total_timesteps, start_time, rollout_time, update_time, 
                    losses, reward_components):
        """ì§„í–‰ìƒí™© ë¡œê¹…"""
        # í†µê³„ ê³„ì‚°
        all_rewards = []
        all_lengths = []
        
        for env_rewards in self.episode_rewards:
            all_rewards.extend(env_rewards[-10:])  # ìµœê·¼ 10ê°œ ì—í”¼ì†Œë“œ
        for env_lengths in self.episode_lengths:
            all_lengths.extend(env_lengths[-10:])
        
        if all_rewards:
            avg_reward = np.mean(all_rewards)
            avg_length = np.mean(all_lengths)
            total_episodes = np.sum(self.episode_counts)
            
            elapsed_time = time.time() - start_time
            fps = total_timesteps / elapsed_time
            
            print(f"ìŠ¤í… {total_timesteps:8,} | ì—í”¼ì†Œë“œ {total_episodes:5,} | "
                  f"í‰ê·  ë³´ìƒ: {avg_reward:7.2f} | í‰ê·  ê¸¸ì´: {avg_length:5.1f} | "
                  f"FPS: {fps:5.0f}")
            
            # ë³´ìƒ êµ¬ì„± ìš”ì†Œ ì¶œë ¥
            if reward_components:
                print("  ë³´ìƒ êµ¬ì„±: ", end="")
                for key, value in sorted(reward_components.items())[:5]:  # ìƒìœ„ 5ê°œë§Œ
                    print(f"{key}: {value:.3f}, ", end="")
                print()
            
            # Weights & Biases ë¡œê¹…
            if self.args.wandb:
                log_dict = {
                    "timestep": total_timesteps,
                    "total_episodes": total_episodes,
                    "avg_reward": avg_reward,
                    "avg_length": avg_length,
                    "fps": fps,
                    "rollout_time": rollout_time,
                    "update_time": update_time,
                }
                
                # ë³´ìƒ êµ¬ì„± ìš”ì†Œ ë¡œê¹…
                for key, value in reward_components.items():
                    log_dict[f"reward/{key}"] = value
                
                if losses:
                    log_dict.update(losses)
                    
                wandb.log(log_dict)
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if avg_reward > self.best_reward:
                self.best_reward = avg_reward
                if not os.path.exists("models"):
                    os.makedirs("models")
                self.agent.save(f"models/best_improved_go2_ppo.pth")
                print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! í‰ê·  ë³´ìƒ: {self.best_reward:.2f}")
    
    def save_models(self, total_timesteps):
        """ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥"""
        if total_timesteps % self.args.save_freq < self.args.num_envs:
            if not os.path.exists("models"):
                os.makedirs("models")
            self.agent.save(f"models/improved_go2_ppo_step_{total_timesteps}.pth")
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {total_timesteps:,} ìŠ¤í…")
    
    def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        if not os.path.exists("models"):
            os.makedirs("models")
        self.agent.save("models/improved_go2_ppo_final.pth")
        
        # í™˜ê²½ ì¢…ë£Œ
        self.vec_env.close()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        # Weights & Biases ì¢…ë£Œ
        if self.args.wandb:
            wandb.finish()


def main():
    parser = argparse.ArgumentParser(description='ê°œì„ ëœ GO2 í•™ìŠµ (Isaac Lab RSL-RL ê¸°ë²•)')
    
    # í™˜ê²½ ì„¤ì •
    parser.add_argument('--num_envs', type=int, default=64,
                        help='ë³‘ë ¬ í™˜ê²½ ìˆ˜')
    parser.add_argument('--no_reference_gait', action='store_true',
                        help='ì°¸ì¡° gait ë¹„í™œì„±í™”')
    parser.add_argument('--render', action='store_true',
                        help='ì²« ë²ˆì§¸ í™˜ê²½ ë Œë”ë§')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--total_timesteps', type=int, default=10000000,
                        help='ì´ í•™ìŠµ ìŠ¤í…')
    parser.add_argument('--rollout_length', type=int, default=8192,
                        help='ë¡¤ì•„ì›ƒ ê¸¸ì´')
    parser.add_argument('--batch_size', type=int, default=2048,
                        help='ë°°ì¹˜ í¬ê¸°')
    
    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--lr', type=float, default=3e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--gamma', type=float, default=0.99, help='í• ì¸ ì¸ìˆ˜')
    parser.add_argument('--gae_lambda', type=float, default=0.95, help='GAE lambda')
    parser.add_argument('--clip_ratio', type=float, default=0.2, help='PPO í´ë¦½ ë¹„ìœ¨')
    parser.add_argument('--ppo_epochs', type=int, default=5, help='PPO ì—…ë°ì´íŠ¸ ì—í¬í¬')
    parser.add_argument('--hidden_dim', type=int, default=512, help='ë„¤íŠ¸ì›Œí¬ íˆë“  ë ˆì´ì–´ í¬ê¸°')
    
    # GPU ìµœì í™”
    parser.add_argument('--mixed_precision', action='store_true', default=True,
                        help='í˜¼í•© ì •ë°€ë„ í›ˆë ¨')
    
    # ë¡œê¹… ë° ì €ì¥
    parser.add_argument('--wandb', action='store_true', help='Weights & Biases ë¡œê¹…')
    parser.add_argument('--save_freq', type=int, default=100000,
                        help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°')
    
    # ê¸°íƒ€
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ')
    
    args = parser.parse_args()
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ì‹œë“œ ì„¤ì •
    if args.seed is not None:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        torch.cuda.manual_seed(args.seed)
    
    print(f"ğŸš€ ê°œì„ ëœ GO2 í•™ìŠµ ì‹œì‘!")
    print(f"ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print(f"  âœ… ê´€ì ˆ ìœ„ì¹˜ ì œì–´ (ì•ˆì •ì )")
    print(f"  âœ… ì°¸ì¡° ë³´í–‰ ìì„¸ ì´ˆê¸°í™”")
    print(f"  âœ… ì•¡ì…˜ ìŠ¤ë¬´ì‹±")
    print(f"  âœ… ëª¨ë“ˆí™”ëœ ë³´ìƒ í•¨ìˆ˜")
    print(f"  âœ… ë°œ ì ‘ì´‰ ì´ë ¥ ì¶”ì ")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ ì‹œì‘
    trainer = ImprovedTrainer(args)
    trainer.train()


if __name__ == "__main__":
    main()