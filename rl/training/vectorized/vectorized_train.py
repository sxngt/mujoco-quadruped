#!/usr/bin/env python3
"""
ë²¡í„°í™” í™˜ê²½ì„ ì‚¬ìš©í•œ ê³ ì† PPO í•™ìŠµ
16ê°œ í™˜ê²½ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ í•™ìŠµ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
"""

import numpy as np
import matplotlib.pyplot as plt
import torch
import gymnasium as gym
from ppo_agent import PPOAgent
from vectorized_env import create_vectorized_env, SyncVectorEnv
import wandb
import argparse
import os
import time
from datetime import datetime


class VectorizedPPOAgent(PPOAgent):
    """
    ë²¡í„°í™” í™˜ê²½ìš© PPO ì—ì´ì „íŠ¸
    ì—¬ëŸ¬ í™˜ê²½ì—ì„œ ë™ì‹œì— ë°ì´í„° ìˆ˜ì§‘
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ë²¡í„°í™” í™˜ê²½ìš© ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.num_envs = None
        self.env_observations = []
        self.env_actions = []
        self.env_rewards = []
        self.env_values = []
        self.env_log_probs = []
        self.env_dones = []
        
    def reset_memory(self):
        """ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        super().reset_memory()
        self.env_observations = []
        self.env_actions = []
        self.env_rewards = []
        self.env_values = []
        self.env_log_probs = []
        self.env_dones = []
    
    def store_vectorized_transition(self, obs, actions, rewards, values, log_probs, dones):
        """
        ë²¡í„°í™”ëœ transition ì €ì¥
        
        Args:
            obs: (num_envs, obs_dim) ê´€ì°°
            actions: (num_envs, action_dim) í–‰ë™
            rewards: (num_envs,) ë³´ìƒ
            values: (num_envs,) ê°€ì¹˜ ì¶”ì •
            log_probs: (num_envs,) ë¡œê·¸ í™•ë¥ 
            dones: (num_envs,) ì¢…ë£Œ í”Œë˜ê·¸
        """
        # ê° í™˜ê²½ë³„ë¡œ ê°œë³„ ì €ì¥
        for i in range(len(obs)):
            self.observations.append(obs[i])
            self.actions.append(actions[i])
            self.rewards.append(rewards[i])
            self.values.append(values[i])
            self.log_probs.append(log_probs[i])
            self.dones.append(dones[i])
    
    def get_vectorized_actions(self, observations):
        """
        ë²¡í„°í™”ëœ ê´€ì°°ì— ëŒ€í•´ í–‰ë™ ìƒì„±
        
        Args:
            observations: (num_envs, obs_dim) ê´€ì°° ë°°ì—´
            
        Returns:
            actions: (num_envs, action_dim) í–‰ë™ ë°°ì—´
            log_probs: (num_envs,) ë¡œê·¸ í™•ë¥  ë°°ì—´  
            values: (num_envs,) ê°€ì¹˜ ì¶”ì • ë°°ì—´
        """
        batch_size = observations.shape[0]
        
        # ë°°ì¹˜ë¡œ ì²˜ë¦¬
        obs_tensor = torch.FloatTensor(observations).to(self.device)
        
        with torch.no_grad():
            # Actor ì¶œë ¥ (í‰ê· , ë¡œê·¸ í‘œì¤€í¸ì°¨)
            action_mean, action_log_std = self.actor(obs_tensor)
            action_std = torch.exp(action_log_std)
            
            # ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§
            dist = torch.distributions.Normal(action_mean, action_std)
            actions = dist.sample()
            log_probs = dist.log_prob(actions).sum(dim=-1)  # ë‹¤ì°¨ì› í–‰ë™ì˜ í•©
            
            # Critic ì¶œë ¥
            values = self.critic(obs_tensor).squeeze(-1)
        
        return actions.cpu().numpy(), log_probs.cpu().numpy(), values.cpu().numpy()


def train_vectorized_ppo(args):
    """
    ë²¡í„°í™” í™˜ê²½ì—ì„œ PPO í•™ìŠµ
    """
    print(f"ğŸš€ ë²¡í„°í™” PPO í•™ìŠµ ì‹œì‘ ({args.num_envs}ê°œ í™˜ê²½)")
    
    # ë²¡í„°í™” í™˜ê²½ ìƒì„±
    if args.sync_env:
        # ë™ê¸°ì‹ í™˜ê²½ (ë””ë²„ê¹…ìš©)
        from vectorized_env import make_env
        env_fns = [make_env(i, None, not args.no_reference_gait) for i in range(args.num_envs)]
        vec_env = SyncVectorEnv(env_fns)
        print("ğŸ”„ ë™ê¸°ì‹ ë²¡í„° í™˜ê²½ ì‚¬ìš©")
    else:
        # ë¹„ë™ê¸°ì‹ í™˜ê²½ (ê³ ì„±ëŠ¥)
        vec_env = create_vectorized_env(
            num_envs=args.num_envs,
            render_mode="human" if args.render else None,
            use_reference_gait=not args.no_reference_gait
        )
        print("âš¡ ë¹„ë™ê¸°ì‹ ë²¡í„° í™˜ê²½ ì‚¬ìš©")
    
    # ë²¡í„°í™” ì—ì´ì „íŠ¸ ìƒì„±
    agent = VectorizedPPOAgent(
        obs_dim=vec_env.observation_space.shape[0],
        action_dim=vec_env.action_space.shape[0],
        lr=args.lr,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        clip_ratio=args.clip_ratio
    )
    
    agent.num_envs = args.num_envs
    
    # ë¡œê¹… ì´ˆê¸°í™”
    if args.wandb:
        wandb.init(
            project="go2-vectorized-locomotion",
            config=vars(args),
            name=f"vec_ppo_{args.num_envs}envs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
    
    # í•™ìŠµ ë³€ìˆ˜
    episode_rewards = [[] for _ in range(args.num_envs)]
    episode_lengths = [[] for _ in range(args.num_envs)]
    episode_counts = np.zeros(args.num_envs, dtype=int)
    best_reward = -np.inf
    
    print(f"ë””ë°”ì´ìŠ¤: {agent.device}")
    print(f"ê´€ì°° ê³µê°„: {vec_env.observation_space}")
    print(f"í–‰ë™ ê³µê°„: {vec_env.action_space}")
    
    # í™˜ê²½ ë¦¬ì…‹
    observations, infos = vec_env.reset()
    current_episode_rewards = np.zeros(args.num_envs)
    current_episode_lengths = np.zeros(args.num_envs)
    
    start_time = time.time()
    timestep = 0
    
    while timestep < args.total_timesteps:
        # ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
        for step in range(args.rollout_length // args.num_envs):
            # ë²¡í„°í™”ëœ í–‰ë™ ìƒì„±
            actions, log_probs, values = agent.get_vectorized_actions(observations)
            
            # í™˜ê²½ ìŠ¤í…
            next_observations, rewards, terminated, truncated, infos = vec_env.step(actions)
            dones = terminated | truncated
            
            # ë°ì´í„° ì €ì¥
            agent.store_vectorized_transition(
                observations, actions, rewards, values, log_probs, dones
            )
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            observations = next_observations
            current_episode_rewards += rewards
            current_episode_lengths += 1
            timestep += args.num_envs
            
            # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬
            for env_idx in range(args.num_envs):
                if dones[env_idx]:
                    episode_rewards[env_idx].append(current_episode_rewards[env_idx])
                    episode_lengths[env_idx].append(current_episode_lengths[env_idx])
                    episode_counts[env_idx] += 1
                    
                    current_episode_rewards[env_idx] = 0
                    current_episode_lengths[env_idx] = 0
            
            # ë Œë”ë§
            if args.render and hasattr(vec_env, 'render'):
                vec_env.render()
        
        # ì •ì±… ì—…ë°ì´íŠ¸
        if len(agent.observations) >= args.rollout_length:
            losses = agent.update_policy(
                n_epochs=args.ppo_epochs,
                batch_size=args.batch_size
            )
            
            # í†µê³„ ê³„ì‚°
            all_rewards = [r for env_rewards in episode_rewards for r in env_rewards[-10:]]
            all_lengths = [l for env_lengths in episode_lengths for l in env_lengths[-10:]]
            
            if all_rewards:
                avg_reward = np.mean(all_rewards)
                avg_length = np.mean(all_lengths)
                total_episodes = np.sum(episode_counts)
                
                elapsed_time = time.time() - start_time
                fps = timestep / elapsed_time
                
                print(f"ìŠ¤í… {timestep:7d} | ì—í”¼ì†Œë“œ {total_episodes:4d} | "
                      f"í‰ê·  ë³´ìƒ: {avg_reward:7.2f} | í‰ê·  ê¸¸ì´: {avg_length:5.1f} | "
                      f"FPS: {fps:5.0f}")
                
                # ë¡œê¹…
                if args.wandb:
                    log_dict = {
                        "timestep": timestep,
                        "total_episodes": total_episodes,
                        "avg_reward": avg_reward,
                        "avg_length": avg_length,
                        "fps": fps,
                        "num_envs": args.num_envs,
                    }
                    if losses:
                        log_dict.update(losses)
                    wandb.log(log_dict)
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                if avg_reward > best_reward:
                    best_reward = avg_reward
                    if not os.path.exists("models"):
                        os.makedirs("models")
                    agent.save(f"models/best_vectorized_go2_ppo.pth")
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥! í‰ê·  ë³´ìƒ: {best_reward:.2f}")
        
        # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸
        if timestep % (args.save_freq * args.num_envs) < args.num_envs:
            if not os.path.exists("models"):
                os.makedirs("models")
            agent.save(f"models/vectorized_go2_ppo_step_{timestep}.pth")
    
    # ìµœì¢… ì €ì¥
    if not os.path.exists("models"):
        os.makedirs("models")
    agent.save("models/vectorized_go2_ppo_final.pth")
    
    # ì •ë¦¬
    vec_env.close()
    if args.wandb:
        wandb.finish()
    
    total_time = time.time() - start_time
    final_fps = timestep / total_time
    
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"ì´ ì‹œê°„: {total_time:.1f}ì´ˆ")
    print(f"í‰ê·  FPS: {final_fps:.0f}")
    print(f"ìµœê³  ì„±ëŠ¥: {best_reward:.2f}")
    
    return agent


def main():
    parser = argparse.ArgumentParser(description='ë²¡í„°í™” PPO í•™ìŠµ')
    
    # ë²¡í„°í™” í™˜ê²½ ì„¤ì •
    parser.add_argument('--num_envs', type=int, default=16, 
                        help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (ê¸°ë³¸: 16)')
    parser.add_argument('--sync_env', action='store_true',
                        help='ë™ê¸°ì‹ í™˜ê²½ ì‚¬ìš© (ë””ë²„ê¹…ìš©)')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--total_timesteps', type=int, default=2000000,
                        help='ì´ í•™ìŠµ ìŠ¤í…')
    parser.add_argument('--rollout_length', type=int, default=4096,
                        help='ë¡¤ì•„ì›ƒ ê¸¸ì´ (num_envsì˜ ë°°ìˆ˜ì—¬ì•¼ í•¨)')
    
    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--lr', type=float, default=3e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--gamma', type=float, default=0.99, help='í• ì¸ ì¸ìˆ˜')
    parser.add_argument('--gae_lambda', type=float, default=0.95, help='GAE lambda')
    parser.add_argument('--clip_ratio', type=float, default=0.2, help='PPO í´ë¦½ ë¹„ìœ¨')
    parser.add_argument('--ppo_epochs', type=int, default=10, help='PPO ì—…ë°ì´íŠ¸ ì—í¬í¬')
    parser.add_argument('--batch_size', type=int, default=256, help='ë°°ì¹˜ í¬ê¸°')
    
    # í™˜ê²½ ì„¤ì •
    parser.add_argument('--no_reference_gait', action='store_true',
                        help='ì°¸ì¡° gait ë¹„í™œì„±í™”')
    parser.add_argument('--render', action='store_true',
                        help='ì²« ë²ˆì§¸ í™˜ê²½ ë Œë”ë§')
    
    # ë¡œê¹… ë° ì €ì¥
    parser.add_argument('--wandb', action='store_true', help='Weights & Biases ë¡œê¹…')
    parser.add_argument('--save_freq', type=int, default=50000,
                        help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸° (ìŠ¤í… ë‹¨ìœ„)')
    
    # ê¸°íƒ€
    parser.add_argument('--seed', type=int, default=None, help='ëœë¤ ì‹œë“œ')
    
    args = parser.parse_args()
    
    # ì‹œë“œ ì„¤ì •
    if args.seed is not None:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(args.seed)
    
    # ë¡¤ì•„ì›ƒ ê¸¸ì´ ìœ íš¨ì„± ê²€ì‚¬
    if args.rollout_length % args.num_envs != 0:
        args.rollout_length = (args.rollout_length // args.num_envs) * args.num_envs
        print(f"âš ï¸  ë¡¤ì•„ì›ƒ ê¸¸ì´ë¥¼ {args.rollout_length}ë¡œ ì¡°ì • (num_envsì˜ ë°°ìˆ˜)")
    
    # í•™ìŠµ ì‹œì‘
    train_vectorized_ppo(args)


if __name__ == "__main__":
    main()