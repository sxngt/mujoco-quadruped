#!/usr/bin/env python3
"""
í†µí•© GO2 í™˜ê²½ì„ ìœ„í•œ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
ì°¸ì¡° ë ˆí¬ì§€í„°ë¦¬ì˜ ì„±ê³µì ì¸ ë°©ë²•ë¡ ì„ GO2ì— ì ìš©
"""

import os
import sys
import numpy as np
import torch
import time
import argparse
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
rl_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(rl_dir)

from environments.integrated import IntegratedGO2Env
from agents.ppo_agent import PPOAgent
import gymnasium as gym


class IntegratedTrainer:
    def __init__(self, 
                 total_timesteps=5_000_000,
                 eval_freq=10_000,
                 save_freq=50_000,
                 log_freq=1000,
                 render_training=False):
        
        self.total_timesteps = total_timesteps
        self.eval_freq = eval_freq
        self.save_freq = save_freq
        self.log_freq = log_freq
        self.render_training = render_training
        
        # í™˜ê²½ ìƒì„± (í›ˆë ¨ ì¤‘ ë Œë”ë§ ì—¬ë¶€ì— ë”°ë¼)
        render_mode = "human" if render_training else None
        self.env = IntegratedGO2Env(render_mode=render_mode)
        
        # PPO ì—ì´ì „íŠ¸ ìƒì„± (ì°¸ì¡° ë ˆí¬ì§€í„°ë¦¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        self.agent = PPOAgent(
            obs_dim=self.env.observation_space.shape[0],
            action_dim=self.env.action_space.shape[0],
            lr=3e-4,           # ì°¸ì¡° ë°©ì‹
            gamma=0.99,        # ì°¸ì¡° ë°©ì‹
            gae_lambda=0.95,   # ì°¸ì¡° ë°©ì‹
            clip_ratio=0.2,    # ì°¸ì¡° ë°©ì‹
            value_coef=0.5,    # ì°¸ì¡° ë°©ì‹
            entropy_coef=0.01, # ì°¸ì¡° ë°©ì‹
            max_grad_norm=0.5, # ì°¸ì¡° ë°©ì‹
            hidden_dim=256     # ì°¸ì¡° ë°©ì‹
        )
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.save_dir = os.path.join(rl_dir, "models", "integrated", f"integrated_go2_{timestamp}")
        os.makedirs(self.save_dir, exist_ok=True)
        
        # ë¡œê·¸ íŒŒì¼
        self.log_file = os.path.join(self.save_dir, "training_log.txt")
        
        # í†µê³„ ì¶”ì 
        self.episode_rewards = []
        self.episode_lengths = []
        self.best_reward = -float('inf')
        
        print(f"=== í†µí•© GO2 í›ˆë ¨ ì‹œì‘ ===")
        print(f"ì´ íƒ€ì„ìŠ¤í…: {total_timesteps:,}")
        print(f"ê´€ì°° ì°¨ì›: {self.env.observation_space.shape[0]}")
        print(f"ì•¡ì…˜ ì°¨ì›: {self.env.action_space.shape[0]}")
        print(f"ì €ì¥ ìœ„ì¹˜: {self.save_dir}")
        print(f"ë””ë°”ì´ìŠ¤: {self.agent.device}")
        print(f"í›ˆë ¨ ì¤‘ ë Œë”ë§: {'âœ… í™œì„±í™”' if self.render_training else 'âŒ ë¹„í™œì„±í™”'}")
        
    def log_message(self, message):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥"""
        print(message)
        with open(self.log_file, 'a') as f:
            f.write(f"{datetime.now()}: {message}\n")
    
    def evaluate_agent(self, num_episodes=5):
        """ì—ì´ì „íŠ¸ í‰ê°€"""
        eval_rewards = []
        eval_lengths = []
        
        for ep in range(num_episodes):
            obs, _ = self.env.reset()
            episode_reward = 0
            episode_length = 0
            
            while True:
                action, _, _ = self.agent.get_action(obs)
                obs, reward, terminated, truncated, info = self.env.step(action)
                
                episode_reward += reward
                episode_length += 1
                
                if terminated or truncated:
                    break
            
            eval_rewards.append(episode_reward)
            eval_lengths.append(episode_length)
        
        avg_reward = np.mean(eval_rewards)
        avg_length = np.mean(eval_lengths)
        
        return avg_reward, avg_length, eval_rewards, eval_lengths
    
    def train(self):
        """ë©”ì¸ í›ˆë ¨ ë£¨í”„"""
        
        obs, _ = self.env.reset()
        total_steps = 0
        episode_num = 0
        episode_reward = 0
        episode_length = 0
        start_time = time.time()
        
        # í›ˆë ¨ íŒŒë¼ë¯¸í„° (ì°¸ì¡° ë°©ì‹)
        rollout_length = 2048  # ì°¸ì¡° ë ˆí¬ì§€í„°ë¦¬ ë°©ì‹
        
        while total_steps < self.total_timesteps:
            
            # Rollout ìˆ˜ì§‘
            for step in range(rollout_length):
                action, log_prob, value = self.agent.get_action(obs)
                next_obs, reward, terminated, truncated, info = self.env.step(action)
                
                # í›ˆë ¨ ì¤‘ ë Œë”ë§ (ìµœì í™”ë¨)
                if self.render_training:
                    # ì„±ëŠ¥ì„ ìœ„í•´ ë§¤ ìŠ¤í…ì´ ì•„ë‹Œ ì ì ˆí•œ ê°„ê²©ìœ¼ë¡œ ë Œë”ë§
                    should_render = (total_steps % 2 == 0)  # 2ìŠ¤í…ë§ˆë‹¤ ë Œë”ë§ (25 FPS ì •ë„)
                    
                    if should_render:
                        if total_steps % 100 == 0:  # 100ìŠ¤í…ë§ˆë‹¤ ìƒíƒœ ë¡œê·¸
                            print(f"ğŸ¬ ë Œë”ë§ ì¤‘... (step {total_steps}, episode {episode_num})")
                        
                        try:
                            render_result = self.env.render()
                            
                            # ì´ˆê¸° ë Œë”ë§ ìƒíƒœ í™•ì¸
                            if total_steps < 10:
                                print(f"  Step {total_steps}: ë Œë”ë§ = {render_result}")
                                if render_result is None:
                                    print(f"    ë·°ì–´ ìƒíƒœ: {type(self.env.viewer)}")
                                    
                            # ë Œë”ë§ ì‹¤íŒ¨ ì‹œ ë·°ì–´ ì¬ì´ˆê¸°í™” ì‹œë„ (í•œ ë²ˆë§Œ)
                            if render_result is None and not hasattr(self, '_viewer_reset_attempted'):
                                print("ğŸ”§ ë·°ì–´ ì¬ì´ˆê¸°í™” ì‹œë„...")
                                self.env.viewer = None
                                self._viewer_reset_attempted = True
                                
                        except Exception as e:
                            if total_steps < 10:
                                print(f"  âŒ Step {total_steps} ë Œë”ë§ ì‹¤íŒ¨: {e}")
                            # ì¹˜ëª…ì ì´ì§€ ì•Šì€ ë Œë”ë§ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                
                # ê²½í—˜ ì €ì¥
                self.agent.store_transition(obs, action, reward, value, log_prob, terminated)
                
                obs = next_obs
                episode_reward += reward
                episode_length += 1
                total_steps += 1
                
                # ì—í”¼ì†Œë“œ ì¢…ë£Œ
                if terminated or truncated:
                    self.episode_rewards.append(episode_reward)
                    self.episode_lengths.append(episode_length)
                    
                    # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                    if episode_reward > self.best_reward:
                        self.best_reward = episode_reward
                        # ìµœê³  ëª¨ë¸ ì €ì¥
                        best_path = os.path.join(self.save_dir, "best_model.pth")
                        self.agent.save(best_path)
                    
                    # ë¡œê·¸ ì¶œë ¥
                    if episode_num % self.log_freq == 0:
                        recent_rewards = self.episode_rewards[-100:] if len(self.episode_rewards) >= 100 else self.episode_rewards
                        avg_reward = np.mean(recent_rewards)
                        elapsed_time = time.time() - start_time
                        
                        message = (f"Episode {episode_num:,} | "
                                 f"Steps: {total_steps:,} | "
                                 f"Reward: {episode_reward:.2f} | "
                                 f"Avg(100): {avg_reward:.2f} | "
                                 f"Best: {self.best_reward:.2f} | "
                                 f"Length: {episode_length} | "
                                 f"Time: {elapsed_time:.1f}s")
                        self.log_message(message)
                        
                        # ìƒì„¸ ì •ë³´ ì¶œë ¥
                        if 'total' in info:
                            detail_msg = (f"  -> Forward: {info.get('lin_vel_reward', 0):.2f} | "
                                        f"Alive: {info.get('alive_reward', 0):.2f} | "
                                        f"Torque: {info.get('torque_cost', 0):.2f}")
                            self.log_message(detail_msg)
                    
                    # ë¦¬ì…‹
                    obs, _ = self.env.reset()
                    episode_num += 1
                    episode_reward = 0
                    episode_length = 0
                
                # í›ˆë ¨ ì¤‘ë‹¨ ì²´í¬
                if total_steps >= self.total_timesteps:
                    break
            
            # ì •ì±… ì—…ë°ì´íŠ¸ (PPO)
            if len(self.agent.observations) > 0:
                update_info = self.agent.update_policy(n_epochs=10, batch_size=64)
                
                if total_steps % (self.log_freq * 10) == 0:
                    update_msg = (f"Policy Update | "
                                f"Policy Loss: {update_info['policy_loss']:.4f} | "
                                f"Value Loss: {update_info['value_loss']:.4f} | "
                                f"Entropy: {update_info['entropy_loss']:.4f}")
                    self.log_message(update_msg)
            
            # í‰ê°€
            if total_steps % self.eval_freq == 0 and total_steps > 0:
                self.log_message("=== í‰ê°€ ì‹œì‘ ===")
                avg_reward, avg_length, eval_rewards, eval_lengths = self.evaluate_agent(num_episodes=5)
                
                eval_msg = (f"Evaluation | "
                          f"Avg Reward: {avg_reward:.2f} | "
                          f"Avg Length: {avg_length:.1f} | "
                          f"Rewards: {[f'{r:.1f}' for r in eval_rewards]}")
                self.log_message(eval_msg)
                self.log_message("=== í‰ê°€ ì™„ë£Œ ===")
            
            # ëª¨ë¸ ì €ì¥
            if total_steps % self.save_freq == 0 and total_steps > 0:
                checkpoint_path = os.path.join(self.save_dir, f"checkpoint_{total_steps}.pth")
                self.agent.save(checkpoint_path)
                self.log_message(f"ëª¨ë¸ ì €ì¥: {checkpoint_path}")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_path = os.path.join(self.save_dir, "final_model.pth")
        self.agent.save(final_path)
        
        # í›ˆë ¨ ì™„ë£Œ ë©”ì‹œì§€
        total_time = time.time() - start_time
        self.log_message("=== í›ˆë ¨ ì™„ë£Œ ===")
        self.log_message(f"ì´ ì‹œê°„: {total_time/3600:.2f}ì‹œê°„")
        self.log_message(f"ì´ ì—í”¼ì†Œë“œ: {episode_num}")
        self.log_message(f"ìµœê³  ë³´ìƒ: {self.best_reward:.2f}")
        self.log_message(f"ìµœì¢… í‰ê·  ë³´ìƒ: {np.mean(self.episode_rewards[-100:]):.2f}")
        
        return self.episode_rewards, self.episode_lengths
    
    def test_trained_model(self, model_path, num_episodes=10, render=True):
        """í›ˆë ¨ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        
        # ëª¨ë¸ ë¡œë“œ
        self.agent.load(model_path)
        self.log_message(f"ëª¨ë¸ ë¡œë“œ: {model_path}")
        
        # ë Œë”ë§ í™˜ê²½ ìƒì„±
        if render:
            test_env = IntegratedGO2Env(render_mode="human")
        else:
            test_env = self.env
        
        test_rewards = []
        test_lengths = []
        
        for episode in range(num_episodes):
            obs, _ = test_env.reset()
            episode_reward = 0
            episode_length = 0
            
            self.log_message(f"=== í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ {episode + 1} ===")
            
            while True:
                action, _, _ = self.agent.get_action(obs)
                obs, reward, terminated, truncated, info = test_env.step(action)
                
                episode_reward += reward
                episode_length += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if episode_length % 100 == 0:
                    pos_x = obs[0] if len(obs) > 0 else 0  # ëŒ€ëµì ì¸ ìœ„ì¹˜
                    forward_vel = info.get('lin_vel_reward', 0)
                    self.log_message(f"  Step {episode_length}: ë³´ìƒ {reward:.2f}, ì „ì§„ë³´ìƒ {forward_vel:.2f}")
                
                if render:
                    test_env.render()
                    time.sleep(0.01)  # ì‹œê°ì  í™•ì¸ì„ ìœ„í•œ ì§€ì—°
                
                if terminated or truncated:
                    break
            
            test_rewards.append(episode_reward)
            test_lengths.append(episode_length)
            
            self.log_message(f"ì—í”¼ì†Œë“œ {episode + 1} ì™„ë£Œ: ë³´ìƒ {episode_reward:.2f}, ê¸¸ì´ {episode_length}")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        avg_reward = np.mean(test_rewards)
        avg_length = np.mean(test_lengths)
        
        self.log_message("=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
        self.log_message(f"í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
        self.log_message(f"í‰ê·  ê¸¸ì´: {avg_length:.1f}")
        self.log_message(f"ëª¨ë“  ë³´ìƒ: {[f'{r:.1f}' for r in test_rewards]}")
        
        if render:
            test_env.close()
        
        return test_rewards, test_lengths


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="í†µí•© GO2 í™˜ê²½ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸")
    
    # í›ˆë ¨ ê´€ë ¨ ì˜µì…˜
    parser.add_argument("--total_timesteps", type=int, default=3_000_000,
                        help="ì´ í›ˆë ¨ íƒ€ì„ìŠ¤í… (ê¸°ë³¸ê°’: 3,000,000)")
    parser.add_argument("--eval_freq", type=int, default=25_000,
                        help="í‰ê°€ ì£¼ê¸° (ê¸°ë³¸ê°’: 25,000)")
    parser.add_argument("--save_freq", type=int, default=100_000,
                        help="ëª¨ë¸ ì €ì¥ ì£¼ê¸° (ê¸°ë³¸ê°’: 100,000)")
    parser.add_argument("--log_freq", type=int, default=50,
                        help="ë¡œê·¸ ì¶œë ¥ ì£¼ê¸° (ê¸°ë³¸ê°’: 50)")
    
    # ë Œë”ë§ ë° í…ŒìŠ¤íŠ¸ ì˜µì…˜
    parser.add_argument("--render", action="store_true",
                        help="í›ˆë ¨ ì¤‘ ì‹¤ì‹œê°„ ë Œë”ë§ í™œì„±í™” (í•™ìŠµ ê³¼ì • ì‹œê°í™”)")
    parser.add_argument("--test_episodes", type=int, default=3,
                        help="í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 3)")
    parser.add_argument("--no_test", action="store_true",
                        help="í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--render_test_only", action="store_true",
                        help="í›ˆë ¨ ì—†ì´ ë Œë”ë§ í…ŒìŠ¤íŠ¸ë§Œ ì¦‰ì‹œ ì‹¤í–‰")
    parser.add_argument("--quick_render", action="store_true",
                        help="ì„ì˜ í–‰ë™ìœ¼ë¡œ ì¦‰ì‹œ ë Œë”ë§ í…ŒìŠ¤íŠ¸ (í›ˆë ¨ ì—†ìŒ)")
    parser.add_argument("--render_steps", type=int, default=None,
                        help="ì§§ì€ í›ˆë ¨ìœ¼ë¡œ ë Œë”ë§ í…ŒìŠ¤íŠ¸ (ì˜ˆ: --render_steps 500)")
    
    # ëª¨ë¸ ë¡œë“œ ê´€ë ¨
    parser.add_argument("--load_model", type=str, default=None,
                        help="ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œì—ì„œ í›ˆë ¨ ê³„ì†í•˜ê¸°")
    parser.add_argument("--test_only", action="store_true",
                        help="í›ˆë ¨ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰")
    
    return parser.parse_args()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±
    args = parse_args()
    
    # ë Œë”ë§ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì§§ì€ í›ˆë ¨ ì˜µì…˜
    if args.render_steps:
        args.total_timesteps = args.render_steps
        args.render = True
        print(f"ğŸ¯ ì§§ì€ ë Œë”ë§ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {args.render_steps} ìŠ¤í…")
    
    # í›ˆë ¨ ì‹œì‘
    trainer = IntegratedTrainer(
        total_timesteps=args.total_timesteps,
        eval_freq=args.eval_freq,
        save_freq=args.save_freq,
        log_freq=args.log_freq,
        render_training=args.render  # í›ˆë ¨ ì¤‘ ë Œë”ë§ ì˜µì…˜ ì „ë‹¬
    )
    
    try:
        # ì¦‰ì‹œ ë Œë”ë§ í…ŒìŠ¤íŠ¸ (í›ˆë ¨ ì—†ìŒ)
        if args.quick_render or args.render_test_only:
            print(f"\n=== ì¦‰ì‹œ ë Œë”ë§ í…ŒìŠ¤íŠ¸ ===")
            test_env = IntegratedGO2Env(render_mode="human")
            obs, _ = test_env.reset()
            
            print("ë¬´ì‘ìœ„ í–‰ë™ìœ¼ë¡œ ë Œë”ë§ í…ŒìŠ¤íŠ¸ ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
            step_count = 0
            
            try:
                while True:
                    # ë¬´ì‘ìœ„ í–‰ë™ (ì•½í•˜ê²Œ)
                    action = test_env.action_space.sample() * 0.3
                    obs, reward, terminated, truncated, info = test_env.step(action)
                    test_env.render()
                    
                    step_count += 1
                    if step_count % 100 == 0:
                        print(f"Step {step_count}, ë³´ìƒ: {reward:.2f}")
                    
                    if terminated or truncated:
                        print(f"ì—í”¼ì†Œë“œ ì¢…ë£Œ (step {step_count}), ë¦¬ì…‹...")
                        obs, _ = test_env.reset()
                        step_count = 0
                    
                    time.sleep(0.02)  # 50 FPS
                    
            except KeyboardInterrupt:
                print(f"\në Œë”ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ({step_count} ìŠ¤í…)")
            finally:
                test_env.close()
            return
        
        # í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰í•˜ëŠ” ê²½ìš° (í›ˆë ¨ëœ ëª¨ë¸)
        elif args.test_only:
            if args.load_model and os.path.exists(args.load_model):
                print(f"\n=== ëª¨ë¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ===")
                trainer.test_trained_model(args.load_model, num_episodes=args.test_episodes, render=args.render)
            else:
                print("í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” --load_model ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return
        else:
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            if args.load_model and os.path.exists(args.load_model):
                trainer.agent.load(args.load_model)
                print(f"ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {args.load_model}")
            
            # í›ˆë ¨ ì‹¤í–‰
            episode_rewards, episode_lengths = trainer.train()
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ê±´ë„ˆë›°ê¸° ì˜µì…˜ì´ ì—†ëŠ” ê²½ìš°)
            if not args.no_test:
                best_model_path = os.path.join(trainer.save_dir, "best_model.pth")
                if os.path.exists(best_model_path):
                    print("\n=== ìµœê³  ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
                    trainer.test_trained_model(best_model_path, num_episodes=args.test_episodes, render=args.render)
        
    except KeyboardInterrupt:
        print("\ní›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í˜„ì¬ê¹Œì§€ì˜ ëª¨ë¸ ì €ì¥
        interrupt_path = os.path.join(trainer.save_dir, "interrupted_model.pth")
        trainer.agent.save(interrupt_path)
        print(f"ì¤‘ë‹¨ëœ ëª¨ë¸ ì €ì¥: {interrupt_path}")
        
    except Exception as e:
        print(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # í™˜ê²½ ì •ë¦¬
        trainer.env.close()
        print("í›ˆë ¨ ì™„ë£Œ ë° ì •ë¦¬")


if __name__ == "__main__":
    main()