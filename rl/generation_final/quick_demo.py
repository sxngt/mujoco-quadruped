#!/usr/bin/env python3
"""
Generation Final ë¹ ë¥¸ ë°ëª¨ - ì§§ì€ í›ˆë ¨ìœ¼ë¡œ ë™ì‘ í™•ì¸
"""

import os
import sys
import numpy as np
import torch
import time

# ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.dirname(current_dir))

from integrated.integrated_go2_env import IntegratedGO2Env
from agents.ppo_agent import PPOAgent

def quick_demo():
    print("ğŸš€ Generation Final - ë¹ ë¥¸ ë°ëª¨ ì‹œì‘")
    print("="*50)
    
    # í™˜ê²½ ìƒì„±
    env = IntegratedGO2Env(render_mode=None)
    print(f"âœ… í™˜ê²½ ìƒì„±: ê´€ì°°({env.observation_space.shape}), ì•¡ì…˜({env.action_space.shape})")
    
    # ì—ì´ì „íŠ¸ ìƒì„± (ì‘ì€ ë„¤íŠ¸ì›Œí¬ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    agent = PPOAgent(
        obs_dim=env.observation_space.shape[0],
        action_dim=env.action_space.shape[0],
        lr=3e-4,
        hidden_dim=64,  # ì‘ê²Œ ì„¤ì •
        gamma=0.99,
        gae_lambda=0.95
    )
    print(f"âœ… PPO ì—ì´ì „íŠ¸ ìƒì„± (ë””ë°”ì´ìŠ¤: {agent.device})")
    
    # ì§§ì€ í›ˆë ¨ ë£¨í”„ (100 ìŠ¤í…ë§Œ)
    obs, _ = env.reset()
    total_reward = 0
    episode_count = 0
    step_count = 0
    
    print("\nğŸ¯ í›ˆë ¨ ì‹œì‘ (100 ìŠ¤í… ë°ëª¨)")
    start_time = time.time()
    
    for step in range(100):
        # ì•¡ì…˜ ì„ íƒ
        action, log_prob, value = agent.get_action(obs)
        
        # í™˜ê²½ ìŠ¤í…
        next_obs, reward, terminated, truncated, info = env.step(action)
        
        # ê²½í—˜ ì €ì¥
        agent.store_transition(obs, action, reward, value, log_prob, terminated)
        
        obs = next_obs
        total_reward += reward
        step_count += 1
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if step % 20 == 0:
            print(f"   Step {step}: ë³´ìƒ={reward:.2f}, ê°’={value:.2f}, ì´ë³´ìƒ={total_reward:.1f}")
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ
        if terminated or truncated:
            episode_count += 1
            print(f"ğŸ’« ì—í”¼ì†Œë“œ {episode_count} ì¢…ë£Œ: ì´ ë³´ìƒ={total_reward:.2f}, ìŠ¤í…={step_count}")
            
            # ì—í”¼ì†Œë“œ ë¦¬ì…‹
            obs, _ = env.reset()
            total_reward = 0
            step_count = 0
    
    elapsed_time = time.time() - start_time
    print(f"\nâ±ï¸ 100 ìŠ¤í… ì™„ë£Œ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    # ì •ì±… ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    if len(agent.observations) > 10:
        print("\nğŸ”„ ì •ì±… ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸...")
        update_info = agent.update_policy(n_epochs=2, batch_size=32)
        print(f"   ì •ì±… ì†ì‹¤: {update_info['policy_loss']:.4f}")
        print(f"   ê°€ì¹˜ ì†ì‹¤: {update_info['value_loss']:.4f}")
        print("âœ… ì •ì±… ì—…ë°ì´íŠ¸ ì„±ê³µ")
    
    # ë³´ìƒ êµ¬ì¡° ë¶„ì„
    print(f"\nğŸ“Š ë³´ìƒ êµ¬ì¡° ë¶„ì„:")
    print(f"   ìµœê·¼ ë³´ìƒ ì •ë³´: {info}")
    
    env.close()
    print("\nğŸ‰ Generation Final ë°ëª¨ ì™„ë£Œ!")
    print("="*50)
    print("âœ… í™˜ê²½ì´ ì •ìƒ ì‘ë™í•˜ë©° í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ")
    print(f"ğŸ’¡ ì „ì²´ í›ˆë ¨: python integrated/train_integrated.py")

if __name__ == "__main__":
    quick_demo()