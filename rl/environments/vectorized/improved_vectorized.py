#!/usr/bin/env python3
"""
ê°œì„ ëœ ë²¡í„°í™” í™˜ê²½ - Isaac Lab RSL-RL ê¸°ë²• ì ìš©
"""

import numpy as np
import time
from improved_environment import ImprovedGO2Env
from typing import List, Tuple, Any, Dict


class ImprovedVectorEnv:
    """
    ê°œì„ ëœ ë²¡í„°í™” í™˜ê²½
    ê´€ì ˆ ìœ„ì¹˜ ì œì–´ì™€ í–¥ìƒëœ ë³´ìƒ í•¨ìˆ˜ ì ìš©
    """
    
    def __init__(self, num_envs: int = 16, render_mode: str = None, use_reference_gait: bool = True):
        self.num_envs = num_envs
        
        # í™˜ê²½ë“¤ ìƒì„±
        self.envs = []
        for i in range(num_envs):
            # ì²« ë²ˆì§¸ í™˜ê²½ë§Œ ë Œë”ë§
            env_render_mode = render_mode if i == 0 else None
            env = ImprovedGO2Env(
                render_mode=env_render_mode,
                use_reference_gait=use_reference_gait
            )
            self.envs.append(env)
        
        # í™˜ê²½ ìŠ¤í™
        self.observation_space = self.envs[0].observation_space
        self.action_space = self.envs[0].action_space
        
        print(f"âœ… {num_envs}ê°œ ê°œì„ ëœ ë²¡í„°í™” í™˜ê²½ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ® ì œì–´ ëª¨ë“œ: ê´€ì ˆ ìœ„ì¹˜ ì œì–´ (PD ì»¨íŠ¸ë¡¤ëŸ¬)")
        print(f"ğŸ“Š ê´€ì°° ê³µê°„: {self.observation_space.shape}")
        print(f"ğŸ¯ í–‰ë™ ê³µê°„: {self.action_space.shape} (ì •ê·œí™”ëœ ê´€ì ˆ ìœ„ì¹˜ ëª…ë ¹)")
        print(f"ğŸƒ ì´ˆê¸°í™”: ì°¸ì¡° ë³´í–‰ ìì„¸")
        print(f"ğŸ’° ë³´ìƒ: ëª¨ë“ˆí™”ëœ ë³´ìƒ í•¨ìˆ˜ (ë°œ ê³µì¤‘ì‹œê°„ ì¶”ì  í¬í•¨)")
    
    def reset(self, seed=None):
        """ëª¨ë“  í™˜ê²½ ë¦¬ì…‹"""
        observations = []
        infos = []
        
        for i, env in enumerate(self.envs):
            env_seed = seed + i if seed is not None else None
            obs, info = env.reset(seed=env_seed)
            observations.append(obs)
            infos.append(info)
        
        return np.array(observations, dtype=np.float32), infos
    
    def step(self, actions):
        """ëª¨ë“  í™˜ê²½ì—ì„œ ë™ì‹œ ìŠ¤í…"""
        observations = []
        rewards = []
        terminated = []
        truncated = []
        infos = []
        
        for env, action in zip(self.envs, actions):
            obs, reward, term, trunc, info = env.step(action)
            observations.append(obs)
            rewards.append(reward)
            terminated.append(term)
            truncated.append(trunc)
            infos.append(info)
        
        return (np.array(observations, dtype=np.float32),
                np.array(rewards, dtype=np.float32),
                np.array(terminated, dtype=bool),
                np.array(truncated, dtype=bool),
                infos)
    
    def render(self):
        """ì²« ë²ˆì§¸ í™˜ê²½ë§Œ ë Œë”ë§"""
        if self.envs and hasattr(self.envs[0], 'render'):
            return self.envs[0].render()
    
    def close(self):
        """ëª¨ë“  í™˜ê²½ ì¢…ë£Œ"""
        for env in self.envs:
            env.close()
        print(f"ğŸ”’ {self.num_envs}ê°œ ê°œì„ ëœ í™˜ê²½ ì¢…ë£Œ ì™„ë£Œ")
    
    def get_reward_info(self):
        """í˜„ì¬ ë³´ìƒ ê°€ì¤‘ì¹˜ ì •ë³´ ë°˜í™˜"""
        if self.envs:
            return self.envs[0].reward_weights
        return {}


def test_improved_environment():
    """ê°œì„ ëœ í™˜ê²½ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê°œì„ ëœ GO2 í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # ë‹¨ì¼ í™˜ê²½ í…ŒìŠ¤íŠ¸
    print("1ï¸âƒ£ ë‹¨ì¼ í™˜ê²½ í…ŒìŠ¤íŠ¸")
    env = ImprovedGO2Env(render_mode="human")
    
    obs, info = env.reset()
    print(f"ì´ˆê¸° ê´€ì°° ì°¨ì›: {obs.shape}")
    print(f"ì´ˆê¸° ë¡œë´‡ ë†’ì´: {env.data.qpos[2]:.3f}m")
    print(f"ì´ˆê¸° ê´€ì ˆ ìœ„ì¹˜: {env.data.qpos[7:19]}")
    
    # ëª‡ ìŠ¤í… ì‹¤í–‰
    total_reward = 0
    reward_breakdown = {}
    
    for step in range(100):
        # ì‘ì€ ëœë¤ ì•¡ì…˜ (ìœ„ì¹˜ ì œì–´)
        action = np.random.uniform(-0.3, 0.3, env.action_space.shape[0])
        obs, reward, terminated, truncated, info = env.step(action)
        
        total_reward += reward
        
        # ë³´ìƒ êµ¬ì„± ìš”ì†Œ ëˆ„ì 
        for key, value in info.items():
            if key not in reward_breakdown:
                reward_breakdown[key] = 0
            reward_breakdown[key] += value
        
        if terminated or truncated:
            print(f"ì—í”¼ì†Œë“œ ì¢…ë£Œ: ìŠ¤í… {step+1}")
            break
        
        if step % 20 == 0:
            print(f"ìŠ¤í… {step}: ë³´ìƒ {reward:.3f}, ë†’ì´ {env.data.qpos[2]:.3f}m")
    
    print(f"\nì´ ë³´ìƒ: {total_reward:.2f}")
    print("ë³´ìƒ êµ¬ì„±ìš”ì†Œ ë¶„ì„:")
    for key, value in reward_breakdown.items():
        print(f"  {key}: {value:.3f}")
    
    env.close()
    
    # ë²¡í„°í™” í™˜ê²½ í…ŒìŠ¤íŠ¸
    print("\n\n1ï¸âƒ£6ï¸âƒ£ ë²¡í„°í™” í™˜ê²½ í…ŒìŠ¤íŠ¸")
    vec_env = ImprovedVectorEnv(num_envs=4, render_mode=None)
    
    observations, infos = vec_env.reset()
    print(f"ê´€ì°° ë°°ì¹˜ í˜•íƒœ: {observations.shape}")
    
    # ëª‡ ìŠ¤í… ì‹¤í–‰
    for step in range(50):
        actions = np.random.uniform(-0.3, 0.3, (4, vec_env.action_space.shape[0]))
        observations, rewards, terminated, truncated, infos = vec_env.step(actions)
        
        if step % 10 == 0:
            print(f"ìŠ¤í… {step}: í‰ê·  ë³´ìƒ {np.mean(rewards):.3f}, "
                  f"ìµœëŒ€ ë³´ìƒ {np.max(rewards):.3f}, ìµœì†Œ ë³´ìƒ {np.min(rewards):.3f}")
    
    vec_env.close()
    
    print("\nâœ… ê°œì„ ëœ í™˜ê²½ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("1. âœ… ê´€ì ˆ ìœ„ì¹˜ ì œì–´ (PD ì»¨íŠ¸ë¡¤ëŸ¬)")
    print("2. âœ… ì°¸ì¡° ë³´í–‰ ìì„¸ ì´ˆê¸°í™”")
    print("3. âœ… ì•¡ì…˜ ìŠ¤ë¬´ì‹±")
    print("4. âœ… ëª¨ë“ˆí™”ëœ ë³´ìƒ í•¨ìˆ˜")
    print("5. âœ… ë°œ ì ‘ì´‰ ì´ë ¥ ì¶”ì ")
    print("6. âœ… ë°œ ê³µì¤‘ ì‹œê°„ ë³´ìƒ")


if __name__ == "__main__":
    test_improved_environment()