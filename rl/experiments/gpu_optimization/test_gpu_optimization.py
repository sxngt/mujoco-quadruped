#!/usr/bin/env python3
"""
GPU ìµœì í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import numpy as np

def test_autocast_deprecation():
    """ìƒˆë¡œìš´ autocast ë¬¸ë²• í…ŒìŠ¤íŠ¸"""
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    device = torch.device('cuda')
    
    # ë”ë¯¸ í…ì„œ ìƒì„±
    x = torch.randn(100, 50, device=device)
    linear = torch.nn.Linear(50, 10).to(device)
    
    print("ğŸ”§ ìƒˆë¡œìš´ autocast ë¬¸ë²• í…ŒìŠ¤íŠ¸")
    
    # ìƒˆë¡œìš´ ë°©ì‹ (ê¶Œì¥)
    try:
        with torch.autocast(device_type='cuda', enabled=True):
            y = linear(x)
        print("âœ… ìƒˆë¡œìš´ autocast ë¬¸ë²• ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ìƒˆë¡œìš´ autocast ë¬¸ë²• ì‹¤íŒ¨: {e}")
    
    # ê¸°ì¡´ ë°©ì‹ (deprecated) - ë¹„êµìš©ìœ¼ë¡œë§Œ ìœ ì§€
    try:
        with torch.autocast(device_type='cuda', enabled=True):
            y = linear(x)
        print("âœ… ê¸°ì¡´ ì½”ë“œë„ ìƒˆë¡œìš´ ë¬¸ë²•ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì—…ë°ì´íŠ¸ëœ autocast ë¬¸ë²• ì‹¤íŒ¨: {e}")

def test_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ” GPU ì •ë³´:")
    print(f"GPU ì´ë¦„: {torch.cuda.get_device_name()}")
    print(f"ì´ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    torch.cuda.empty_cache()
    initial_memory = torch.cuda.memory_allocated() / 1e9
    print(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.3f}GB")
    
    # í° í…ì„œ ìƒì„±
    large_tensor = torch.randn(10000, 10000, device='cuda')
    after_memory = torch.cuda.memory_allocated() / 1e9
    print(f"í° í…ì„œ ìƒì„± í›„: {after_memory:.3f}GB (ì¦ê°€: {after_memory - initial_memory:.3f}GB)")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del large_tensor
    torch.cuda.empty_cache()
    final_memory = torch.cuda.memory_allocated() / 1e9
    print(f"ì •ë¦¬ í›„: {final_memory:.3f}GB")

def test_mixed_precision():
    """í˜¼í•© ì •ë°€ë„ í…ŒìŠ¤íŠ¸"""
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    device = torch.device('cuda')
    
    # ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬
    model = torch.nn.Sequential(
        torch.nn.Linear(100, 256),
        torch.nn.ReLU(),
        torch.nn.Linear(256, 256),
        torch.nn.ReLU(),
        torch.nn.Linear(256, 10)
    ).to(device)
    
    x = torch.randn(64, 100, device=device)
    target = torch.randn(64, 10, device=device)
    
    # GradScaler ìƒì„± (ìƒˆë¡œìš´ ë¬¸ë²•)
    scaler = torch.amp.GradScaler('cuda')
    optimizer = torch.optim.Adam(model.parameters())
    
    print("ğŸ”¥ í˜¼í•© ì •ë°€ë„ í›ˆë ¨ í…ŒìŠ¤íŠ¸")
    
    # í˜¼í•© ì •ë°€ë„ë¡œ ëª‡ ë²ˆì˜ forward/backward
    for i in range(5):
        optimizer.zero_grad()
        
        with torch.autocast(device_type='cuda', enabled=True):
            output = model(x)
            loss = torch.nn.functional.mse_loss(output, target)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        print(f"ìŠ¤í… {i+1}: ì†ì‹¤ {loss.item():.4f}")
    
    print("âœ… í˜¼í•© ì •ë°€ë„ í›ˆë ¨ ì„±ê³µ")

if __name__ == "__main__":
    print("ğŸ§ª GPU ìµœì í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print()
    
    test_autocast_deprecation()
    print()
    
    test_gpu_memory()
    print()
    
    test_mixed_precision()
    print()
    
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")